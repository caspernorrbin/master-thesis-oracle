As seen in Section~\ref{sec:buddyadditions}, there is ample room to improve the original binary buddy allocator. The two designs discussed have different strengths, so both are implemented to compare and contrast with each other and the original design. The binary tree allocator is closer to the binary buddy allocator in how it allocates, whereas the ibuddy allocator is intrinsically different. Ideally, the allocation and deallocation processes should be most efficient for the most frequently requested allocation size, which is why both allocators are investigated.

Further enhancements can then be developed that are orthogonal to the specific allocator implementations. Such enhancements can be either general optimizations or optimizations specifically tailored to the context of operating within ZGC. Constricting the use case enables the allocators to assume access to additional information or to narrow down their functionality.

\subsection{Use Cases}
There exist numerous scenarios in which a garbage collector could benefit from an advanced memory allocator. There are likely many that will not be brought up in this thesis. Each scenario has its own distinct characteristics with unique requirements. Therefore, any improvements made should enhance the specific scenario without causing adverse effects in other situations. In case a modification does have negative repercussions on other scenarios, it must be possible to disable that change so that each scenario can utilize the most suitable configuration for its needs.

\newpage
Making the allocator configurable in this way improves its adaptability to a wide range of scenarios. Although this versatility is advantageous, it may lead to limitations in certain instances. Prioritizing configurability hinders the allocator from being fully optimized for one particular use case. Implementing significant modifications to enhance one specific scenario could have adverse effects on others. Such changes were not taken into account during the allocator's adaptation as a result of their perceived narrow focus.

\subsection{Allocator Metadata Implementations} \label{sec:adaptationsmetadata}
The binary tree allocator keeps the design of the original binary buddy allocator, but stores its metadata in a different format. This allows for faster operations, as no explicit free-list is kept that needs to be inserted and removed from.

The binary tree is stored as a flattened byte array. The start of the array aligns with the start of the first level, with each subsequent level stored contiguously following the preceding one. Although most levels have each byte representing the value of a single block, the lowest levels deviate from this pattern. Given that each node in the tree holds the maximum possible level that can be allocated within it or its children, the smallest blocks can only store 1 or 0. For the lowest level, memory usage can thus be optimized by compacting and storing 8 blocks within each byte in the array. As this level comprises half of the total blocks, this results in a memory saving of $\frac{1}{2}\times\frac{7}{8}=43.75\%$. Similarly, the data for the subsequent two levels can be compacted to 2 bits per block. This further reduces memory by $\frac{1}{4}\times\frac{6}{8}+\frac{1}{8}\times\frac{6}{8}=28.13\%$. Levels above these require at least 4 bits of information and constitute a smaller fraction of total blocks. The decision was made not to optimize the memory of these levels, as the performance impact of having to perform bit operations at each block would outweigh the negligible improvement in memory usage.

\subsection{Allocating on Already Used Memory} \label{sec:freerangeexpl}
There may be situations in which the use of an allocator may not be the most efficient option. For instance, one could opt for bump-pointer allocation initially and then switch to a more advanced allocator when significant external fragmentation arises. This approach allows for maximizing the speed advantage of bump-pointer allocations for as long as possible. In the context of ZGC, this means that the allocator is used selectively in situations where it is considered more effective than the usual bump-pointer operation.

\newpage
Normally, an allocator has full authority over its allocatable memory region, allowing it to store data and allocate memory locations as needed. However, in this particular scenario, the allocator must be able to allocate around occupied chunks within its memory area. This imposes certain limitations on the allocator, specifically preventing it from assuming that any random memory is available for its utilization. This affects the possibilities of where to store the allocator metadata, which is commonly stored within its memory region.

When working with previously allocated memory, the allocator is initially fully occupied, leaving no available locations for allocation. The allocator then requires the user to indicate either the specific locations of the occupied memory or the intervals between them. This process essentially involves the user defining free regions within the memory space that the allocator can utilize. In ZGC, the live analysis of a page contains information on live objects and can be used to find the specific memory locations that are fixed, enabling the deallocation of the intervals between them.

To implement this, the initial internal state of the full allocator is such that the entire memory is filled with smallest-sized blocks. This means that all blocks are split initially, which avoids accidentally merging blocks that overlap with occupied memory regions. When deallocating a specific range, the largest blocks that can fit into that range are added to the free-list, and each block and its split children are marked as no longer split. Figure~\ref{fig:deallocrange} shows an example of this, where only the last block is occupied. We can see that the blocks added to the free-list cover the entire free range and are of the maximum possible size.

\begin{figure}[h]
    \centering
    \includesvg[width=0.7\textwidth]{figures/deallocrange.svg}
    \caption{The blocks added to the free-list after deallocating the range up until the last block.}
    \label{fig:deallocrange}
\end{figure}

\subsection{Finding Block Buddies} \label{sec:findbuddiesexpl}
When deallocating, the allocator must determine the correct block to deallocate, given the memory address. However, a particular address can come from any block level. The simplest solution is to require the user to provide additional information about the allocated size, and then use that to calculate from which level to deallocate from. This is very fast and, if possible for the user, leads to the lowest overhead.

Another solution is to store the level of each allocation, either inside the block or outside it, in a separate data structure. This is not very memory efficient, but is simple and lookup is fast. When the data is stored within a block, the usable space becomes smaller and of inconvenient size. If the data is stored separately, each possible block location can instead be stored, resulting in a slightly larger but constant overhead.

A third way is to use a bitmap to track which blocks have been split. With this information, it is possible to deduce the size of a block at a particular address. This is very memory-efficient, but more operations are needed to find the correct blocks. Figure~\ref{fig:buddybmapsplit} shows how this bitmap looks after the same previous 16-byte block allocation. We can see that all blocks above that block have been marked as split. We can also see that the smallest blocks do not require an entry in the bitmap as they cannot be split. When a block is deallocated, the allocator will start at the top of the bitmap, then traverse down until the block is no longer marked as split.

\begin{figure}[h]
    \centering
    \includesvg[width=0.7\textwidth]{figures/bbuddy_bmap_split.svg}
    \caption{The state of the split blocks-bitmap and free-list of a binary buddy allocator after one 16-byte allocation.}
    \label{fig:buddybmapsplit}
\end{figure}

All the options mentioned are implemented and can be used in the allocators.  The most suitable option depends on the specific usage scenario, each having trade-offs between memory usage and processing speed. For the most general use case, the most efficient selection would be the bitmap approach, which is very memory efficient across many allocator configurations.

For use within ZGC, it is possible to omit the data structures responsible for tracking size and delegate this function to the garbage collector. The collector possesses sufficient data from its live analysis and the object headers to compute the size of each object and allocation. Given that the data is easily accessible, there is no need for the allocator to incur additional overhead, instead making optimal use of existing resources.

\subsection{Lazy Splitting and Merging of Blocks} \label{sec:lazyexpl}
Splitting and merging blocks is costly and should therefore be avoided if possible. The binary buddy allocator merges blocks whenever possible, which often results in the need to immediately split blocks on subsequent allocation. Lee and Barkley \cite{lazylayer} have designed a modified buddy allocator that dynamically delays the merging of blocks. As long as the allocation size distribution stays consistent, one can avoid the costs related to splitting and merging. To reduce this overhead in the modified allocators, a simpler version of Lee and Barkley's design is implemented. A second free-list (the lazy layer) is created on top of the buddy allocator (the buddy layer). This separate free-list is independent of the free-list of the buddy layer, and it does not perform any splitting or merging; it solely stores blocks of different sizes.

When deallocating using a lazy layer, the block is inserted into the free-list of the lazy layer instead of the buddy layer. However, if the size of the lazy layer reaches a certain threshold of blocks already in the lazy layer, the block is inserted into the buddy layer as normal. During allocation, the lazy layer is first checked for a block of the correct size. Only if this fails, the blocks from the buddy layer are split. If both of these steps fail, the lazy layer can be emptied to allow for the merging of smaller blocks to meet the required allocation size.

It is logical for different block sizes to have different thresholds, as the frequency of sizes differ. Since most allocations are small, it is also logical that that size should have a higher threshold. Storing large blocks in the lazy layer would also reserve these large blocks for only that size, which could lead to smaller allocations not being able to be fulfilled. When implementing this, the smallest block size has the highest threshold, with each level above that halving the threshold of the previous level. The default threshold is set to 1000. A high value is desired, as ZGC can often deallocate many objects to then allocate many objects of the same size. The lazy layer needs the capacity to support this, which is why the threshold is set relatively high.

% \subsection{Skewed Allocation Distribution} \label{sec:skewedexpl}
% In the binary buddy allocator, smaller blocks require more work due to the need for more splits and more potential merges. This contrasts with real-world scenarios, where smaller allocations are more frequent and larger ones are less common. Ideally, the allocation and deallocation processes should be most efficient for the most commonly requested allocation size.

\subsection{Allocator Regions} \label{sec:concurrencyexpl}

In the original binary buddy allocator, the size of the largest block is equal to the entire provided memory, and is then immediately split down into smaller blocks when allocating. A consequence of this is that when less memory is used, the largest blocks become very large. This is a problem in multithreaded programs, where many allocations can happen concurrently. Only one allocation can split the large block, leading to degraded performance with more concurrent allocations.

A way to avoid this is to set the maximum block size lower than the entire memory size and then have it consist of many regions of the same size, with each region containing the normal buddy block structure. Park et al.~\cite{park2014ibuddy} suggest doing this for the iBuddy allocator. For example, one could split the memory and have 2 regions, each having a maximum block size of $\frac{1}{2}$th the entire memory size, which would allow for two allocations concurrently. Diving the memory into regions would both limit the possible number of merges, increasing performance, and make the memory less fragmented if allocations to some regions are prioritized.

The maximum allocation size in a small ZGC page is 256 KB, which is a fraction of the page size of 2 MB. This allows for 8 allocations of the maximum size within a page. Therefore, the allocator is not required to handle blocks that exceed $\frac{1}{8}$th of the total memory size. Instead, the allocator can be divided into eight distinct regions, each covering $\frac{1}{8}$th of a page.

Each of these areas operate independently of all others, enabling them to operate concurrently without the need for extra logic. As long as the allocations are distributed evenly across the regions, they can run concurrently. This scalability extends up to the number of regions, with further allocations having to wait until a prior allocation within a region is completed.

There may be instances where certain regions fill more than others, leading to a decrease in overall potential throughput. When a thread initiates an allocation, it is assigned a specific region. If another thread is currently making an allocation in that region, it proceeds to check the subsequent regions. If all regions are already in use, the thread must wait for the allocation process in a region to finish. When entering a region, the necessary allocation logic is performed. In the case where an allocation is unsuccessful, for instance, due to insufficient space in the region, the thread exits that region and retries in the next one. A request for allocation fails when all regions are unable to accommodate the request.

% what is the problem
% how can it be solved
% why is it better