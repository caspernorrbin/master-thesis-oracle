\subsection{Integration Into ZGC} \label{sec:futureworkZ}

The most significant future step is to integrate the allocators into ZGC. As highlighted in Section~\ref{sec:individual_contrubitons}, Gärds has concurrently to this thesis explored the integration of an allocator into ZGC~\cite{niclas}, focusing on a free-list-based allocator to enhance the efficiency of relocating objects within ZGC. Future efforts could build on Gärds' research by incorporating a buddy allocator into ZGC or exploring alternative integration scenarios not considered by Gärds.

When integrating a buddy allocator specifically, the garbage collector can implement strategies to mitigate some of its drawbacks. The primary issue is the considerable internal fragmentation during allocation. To minimize this, ZGC could allocate multiple objects simultaneously to align more closely with size requirements or use fragmented memory from an allocation for future use. This approach requires that objects allocated together be deallocated collectively. However, as detailed in Section~\ref{sec:freerangeexpl}, this no longer becomes a consideration when deallocating ranges.

\subsection{Smaller Minimum Allocation Size} \label{sec:futureworkLiliput}
Currently, the minimum allocation size in ZGC is 16 bytes, constrained by the header size of Java objects. Efforts are underway to decrease the size of these headers, potentially lowering the smallest object size to 8 bytes \cite{liliput}. Since this development is ongoing, it was not considered in this thesis.

Reducing the minimum allocation size would introduce a new bottom layer to the allocators. Practically, this results in memory overhead per page doubling as it becomes necessary to manage and track smaller allocations. Additionally, for the binary buddy and iBuddy allocators, storing the two pointers needed for the free-list would no longer be feasible. These pointers would need to be transformed into offsets from the beginning of the page. Although the size of pages makes this adjustment manageable, it would still introduce some performance overhead.

\subsection{Further Allocation Optimizations} \label{sec:futureworkOptimizations}
Improving the binary tree storage format in the binary tree allocator might be feasible. Currently, the levels of the tree are stored sequentially, requiring progressively larger jumps when iterating the levels. To increase spatial locality, the larger tree could be broken down into several subtrees stored consecutively. This would place the segments closer together, reducing the likelihood of cache misses and potentially increasing performance.

Currently, all the adapted allocators use the same concurrency approach, which involves dividing memory into regions. However, since their designs differ, it would be worth investigating different designs for each allocator. For example, a lock-free mechanism could achieve concurrency for the binary tree allocator. As the binary tree allocator navigates its tree structure, compare-and-swap (CAS) atomic operations could replace the current subtraction operations. This change would require increased caution, and the allocator would need to reserve more space during tree traversal to ensure it can allocate as expected. This space would be returned after the operation is complete. Conflicts could arise with other threads, but threads can back off to resolve these conflicts. Consequently, retaining the region implementation is advisable to distribute allocations evenly while removing the locking code.

% alla samma, dåligt
% undersöka anpassing för specifika versioner
% binary tree extra bra!!! därför att......
% 

